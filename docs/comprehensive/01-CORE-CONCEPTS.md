# CORE CONCEPTS EXPLAINED
## From Zero to Expert: Understanding Every Technology in Rig Alpha

**Document 01 of 09**  
**Reading Time:** 60-90 minutes  
**Level:** Beginner to Advanced

---

## ğŸ“‹ TABLE OF CONTENTS

1. [Industrial IoT Fundamentals](#1-industrial-iot-fundamentals)
2. [Message Streaming with Apache Kafka](#2-message-streaming-with-apache-kafka)
3. [Machine Learning for Anomaly Detection](#3-machine-learning-for-anomaly-detection)
4. [AI and Natural Language Processing](#4-ai-and-natural-language-processing)
5. [Database Technologies](#5-database-technologies)
6. [Frontend and 3D Visualization](#6-frontend-and-3d-visualization)

---

## 1. INDUSTRIAL IoT FUNDAMENTALS

### 1.1 What is IoT?

**IoT** stands for **Internet of Things**.

**The Simple Explanation:**
IoT means connecting everyday physical objects to the internet so they can send and receive data. Think of a smart thermostat in your home - it's a "thing" (thermostat) connected to the "internet" so you can control it from your phone.

**The Technical Definition:**
IoT is a network of physical devices embedded with **sensors**, **software**, and **connectivity** that enables these objects to collect and exchange data with other devices and systems over the internet.

**Real-World Examples:**
- **Smart Home**: Thermostat adjusts temperature based on your location
- **Wearables**: Fitness tracker counts your steps and monitors heart rate
- **Smart Cars**: Tesla monitors battery health and updates software remotely
- **Industrial IoT**: Factory machines report their status continuously

### 1.2 What is Industrial IoT (IIoT)?

**Industrial IoT** is IoT applied to manufacturing, energy, transportation, and other industrial sectors.

**Key Differences from Consumer IoT:**

| Consumer IoT | Industrial IoT |
|--------------|----------------|
| Smart refrigerator | Factory production line |
| Convenience focus | Safety and efficiency critical |
| Failure = Annoyance | Failure = Millions in losses |
| Data every few minutes | Data every second |
| WiFi is fine | Needs ultra-reliable networks |
| 1-10 devices | Thousands of devices |

**Why IIoT Matters:**
- **Downtime costs $260,000/hour** in automotive manufacturing
- **Predictive maintenance reduces costs by 30-50%**
- **Equipment lifespan increases by 20-40%**
- **Unplanned downtime reduced by 70%**

### 1.3 What are Industrial Sensors?

A **sensor** is a device that detects and measures physical properties and converts them into signals that can be read by an observer or instrument.

**Sensor Categories in Rig Alpha:**

#### **Environmental Sensors**
Measure conditions in the surrounding environment:
- **Temperature**: How hot or cold something is (Â°F or Â°C)
- **Pressure**: Force applied over an area (PSI)
- **Humidity**: Amount of water vapor in air (%)
- **Air Quality**: Concentration of pollutants (AQI index)

**Why they matter:** Environmental conditions affect equipment performance and worker safety.

#### **Mechanical Sensors**
Measure motion, force, and mechanical properties:
- **Vibration**: Oscillating motion (mm/s)
- **RPM**: Rotational speed (revolutions per minute)
- **Torque**: Rotational force (Nm)
- **Alignment**: Position accuracy (mm)

**Why they matter:** Mechanical failures are the #1 cause of industrial breakdowns.

#### **Thermal Sensors**
Measure heat and temperature at various points:
- **Coolant Temperature**: Fluid keeping system cool
- **Bearing Temperature**: Heat from friction in rotating parts
- **Exhaust Temperature**: Heat from combustion or process
- **Oil Temperature**: Lubricant temperature

**Why they matter:** Overheating causes 30% of all equipment failures.

#### **Electrical Sensors**
Measure electrical properties:
- **Voltage**: Electrical pressure (V)
- **Current**: Flow of electricity (A)
- **Power Factor**: Efficiency of power usage (0-1)
- **Frequency**: Alternating current cycles per second (Hz)

**Why they matter:** Electrical problems can cause fires and equipment damage.

#### **Fluid Dynamics Sensors**
Measure properties of flowing liquids:
- **Flow Rate**: Volume per time (L/min)
- **Viscosity**: Resistance to flow (cP)
- **Pump Efficiency**: How well pump works (%)
- **Pressure Drop**: Pressure loss through pipes (PSI)

**Why they matter:** Fluid system failures cause process disruptions and quality issues.

### 1.4 Why Do Factories Need Monitoring Systems?

**Problem 1: Equipment is Expensive**
- A single CNC machine: $500,000
- A production line: $5-50 million
- An oil refinery: $1-10 billion

**Problem 2: Downtime is Catastrophic**
- Automotive manufacturing: $260,000/hour lost
- Semiconductor fab: $2,000,000/hour lost
- Oil refinery: $5,000,000/hour lost

**Problem 3: Manual Monitoring is Impossible**
- Modern factory: 10,000+ sensors
- Data generated: Terabytes per day
- Human can't watch everything 24/7

**Problem 4: Failures Cascade**
- One bearing fails â†’ Motor overheats
- Motor fails â†’ Drive belt breaks
- Belt breaks â†’ Entire line stops
- **Total cost: 100x the bearing replacement cost**

**The Solution: Automated Monitoring**
Rig Alpha monitors everything, detects problems early, and prevents cascading failures.

### 1.5 What is Anomaly Detection?

**Anomaly** = Something unusual or unexpected

**Anomaly Detection** = Finding patterns in data that don't match expected behavior

**Visual Example:**
```
Normal sensor readings over time:
Temperature: 70, 71, 69, 70, 72, 71, 70
                â†“
Anomaly appears:
Temperature: 70, 71, 69, 70, 95, 71, 70
                              ^^
                           ANOMALY!
```

**Why It Matters:**
- **Early Warning**: Catch problems before catastrophic failure
- **Prevent Damage**: Stop cascade failures
- **Save Money**: $2,000 planned repair vs $200,000 emergency
- **Safety**: Prevent accidents and injuries

**Types of Anomalies Rig Alpha Detects:**

1. **Point Anomalies**: Single unusual value
   - Example: Temperature suddenly spikes to 200Â°F

2. **Contextual Anomalies**: Value unusual in context
   - Example: High RPM is normal during production, but not at 3 AM when machine should be idle

3. **Collective Anomalies**: Pattern of values is unusual
   - Example: Temperature slowly rising over 3 days (gradual bearing failure)

---

## 2. MESSAGE STREAMING WITH APACHE KAFKA

### 2.1 What is Apache Kafka?

**Apache Kafka** is an open-source distributed event streaming platform.

**Translation:** Kafka is a super-fast postal service for digital messages that can handle millions of messages per second and never loses any mail.

**Created by:** LinkedIn in 2011, now maintained by Apache Software Foundation

**Used by:** Netflix, Uber, Airbnb, LinkedIn, Microsoft, NASA, and thousands of companies

### 2.2 Why Do We Need Kafka?

**The Problem Without Kafka:**

Imagine our factory system without a message broker:

```
Producer (Sensor Data Generator)
    â†“ Direct Connection
Database â† Consumer trying to write

Problems:
- If database is slow, producer must wait
- If database crashes, data is lost
- If consumer crashes, producer must stop
- Can't process data in parallel
- No replay capability
- Tight coupling between components
```

**The Solution With Kafka:**

```
Producer â†’ Kafka (Message Queue) â†’ Consumer â†’ Database

Benefits:
- Producer never waits for consumer
- Messages stored safely in Kafka
- If consumer crashes, messages wait in queue
- Multiple consumers can process in parallel
- Can replay old messages
- Loose coupling = easier to maintain
```

**Real-World Analogy:**

**Without Kafka** = Handing packages directly to recipient
- Must wait for them to answer door
- If they're not home, package is lost
- Can only deliver to one person at a time

**With Kafka** = Dropping packages at post office
- Drop off and leave immediately
- Post office keeps package safe
- Multiple delivery drivers can work in parallel
- If delivery fails, package stays at post office

### 2.3 Kafka Core Concepts

#### **Topic**

**Definition:** A category or feed name where messages are published

**Analogy:** Topics are like channels on TV or folders in email

**Example in Rig Alpha:**
- Topic name: `sensor-data`
- Contains: All sensor readings from all machines

**Why multiple topics?**
- `sensor-data`: Raw sensor readings
- `anomalies`: Detected anomalies only
- `alerts`: Critical alerts needing immediate action

#### **Partition**

**Definition:** A topic is divided into partitions for parallel processing

**Visual Representation:**
```
Topic: sensor-data
â”œâ”€â”€ Partition 0: [msg1, msg4, msg7, msg10]
â”œâ”€â”€ Partition 1: [msg2, msg5, msg8, msg11]
â”œâ”€â”€ Partition 2: [msg3, msg6, msg9, msg12]
```

**Why partitions?**
- **Scalability**: Multiple consumers read different partitions simultaneously
- **Parallelism**: Process messages faster
- **Ordering**: Messages within a partition are ordered

**Example:**
- 3 partitions â†’ 3 consumers can work in parallel
- 10 partitions â†’ 10 consumers = 10x faster processing

#### **Producer**

**Definition:** An application that publishes messages to Kafka topics

**In Rig Alpha:** `producer.py` generates sensor data and publishes to Kafka

**Producer's Job:**
1. Generate or collect data (sensor readings)
2. Format data as JSON message
3. Send to Kafka topic
4. Wait for acknowledgment
5. Retry if failed

**Code Flow:**
```python
# Simplified producer workflow
sensor_reading = generate_sensor_data()
message = json.dumps(sensor_reading)
producer.send('sensor-data', value=message)
```

#### **Consumer**

**Definition:** An application that reads messages from Kafka topics

**In Rig Alpha:** `consumer.py` reads sensor data from Kafka and processes it

**Consumer's Job:**
1. Subscribe to topic(s)
2. Poll for new messages
3. Process each message
4. Commit offset (mark message as processed)

**Code Flow:**
```python
# Simplified consumer workflow
for message in consumer:
    sensor_data = json.loads(message.value)
    process_data(sensor_data)
    consumer.commit()  # Mark as processed
```

#### **Consumer Group**

**Definition:** Multiple consumers working together to process messages

**Benefit:** Parallel processing for faster throughput

**Example:**
```
Topic: sensor-data (3 partitions)

Consumer Group: sensor-consumer-group
â”œâ”€â”€ Consumer A â†’ Reads Partition 0
â”œâ”€â”€ Consumer B â†’ Reads Partition 1
â””â”€â”€ Consumer C â†’ Reads Partition 2

Result: 3x faster processing!
```

#### **Offset**

**Definition:** A unique sequential ID for each message in a partition

**Analogy:** Page numbers in a book

**Why offsets matter:**
- Know where you stopped reading
- Can replay from any point
- Exactly-once processing guarantee

**Visual:**
```
Partition 0:
Offset: 0    1    2    3    4    5    6
Msg:   [A] [B] [C] [D] [E] [F] [G]
              â†‘
         Consumer at offset 2
         Next read: offset 3
```

#### **Broker**

**Definition:** A Kafka server that stores and serves messages

**In Rig Alpha's Docker setup:**
- Broker address: `localhost:9092`
- Stores messages on disk
- Handles producer writes and consumer reads

**Kafka Cluster:**
```
Multiple Brokers for High Availability:

Broker 1 (Leader for Partition 0)
â”œâ”€â”€ Replica of Partition 1
â””â”€â”€ Replica of Partition 2

Broker 2 (Leader for Partition 1)
â”œâ”€â”€ Replica of Partition 0
â””â”€â”€ Replica of Partition 2

Broker 3 (Leader for Partition 2)
â”œâ”€â”€ Replica of Partition 0
â””â”€â”€ Replica of Partition 1

If Broker 1 fails:
â†’ Broker 2 becomes leader for Partition 0
â†’ No data lost, no downtime!
```

### 2.4 Exactly-Once Semantics

**The Problem:**

What happens if a message is sent but the acknowledgment is lost?

```
Scenario 1: At-Most-Once (Might Lose Data)
Producer â†’ Message â†’ Kafka âœ“
                    â†“ Network failure
Producer â† Ack âœ— (never received)
Producer: "Failed! Don't retry"
Result: Message lost

Scenario 2: At-Least-Once (Might Duplicate)
Producer â†’ Message â†’ Kafka âœ“
                    â†“ Network failure
Producer â† Ack âœ— (never received)
Producer: "Failed! Retry"
Producer â†’ Message (again) â†’ Kafka âœ“
Result: Duplicate message!

Scenario 3: Exactly-Once (Perfect)
Producer â†’ Message (with unique ID) â†’ Kafka âœ“
                                     â†“ Network failure
Producer â† Ack âœ— (never received)
Producer: "Failed! Retry"
Producer â†’ Message (same ID) â†’ Kafka checks ID
Kafka: "Already have this message, ignore"
Result: Message delivered exactly once!
```

**How Rig Alpha Achieves Exactly-Once:**

1. **Producer side:**
   - Enables idempotence (duplicate detection)
   - `acks='all'` - waits for all replicas to confirm
   - Retries on failure

2. **Consumer side:**
   - Disables auto-commit
   - Manually commits only after successful database write
   - If crash before commit, message reprocessed (idempotent operations)

**Code in config.py:**
```python
KAFKA_PRODUCER_CONFIG = {
    'acks': 'all',  # Wait for all replicas
    'retries': 5,   # Retry failed sends
    'max_in_flight_requests_per_connection': 1  # Preserve ordering
}

KAFKA_CONSUMER_CONFIG = {
    'enable_auto_commit': False  # Manual commit for exactly-once
}
```

### 2.5 Exponential Backoff

**Definition:** A retry strategy where wait time increases exponentially after each failure

**Why We Need It:**

Without backoff:
```
Try 1: Failed - wait 1s
Try 2: Failed - wait 1s
Try 3: Failed - wait 1s
...
(Hammering a failing service with requests)
```

With exponential backoff:
```
Try 1: Failed - wait 1s
Try 2: Failed - wait 2s (2^1)
Try 3: Failed - wait 4s (2^2)
Try 4: Failed - wait 8s (2^3)
Try 5: Failed - wait 16s (2^4)
...
(Give service time to recover)
```

**In Rig Alpha:**
```python
INITIAL_RETRY_DELAY = 1
MAX_RETRY_DELAY = 60
BACKOFF_MULTIPLIER = 2

Current delay = min(
    INITIAL_RETRY_DELAY * (BACKOFF_MULTIPLIER ^ attempt),
    MAX_RETRY_DELAY
)
```

**Real-World Example:**
- Kafka broker is restarting (takes 30 seconds)
- Without backoff: 30 failed connection attempts in 30 seconds
- With backoff: 5 attempts (1s, 2s, 4s, 8s, 15s), then success!

### 2.6 Why Kafka Over Direct Database Writes?

**Comparison:**

| Feature | Direct DB | With Kafka |
|---------|-----------|-----------|
| **Speed** | Slow (wait for DB) | Fast (fire and forget) |
| **Reliability** | Lost if DB down | Stored in Kafka |
| **Scalability** | DB bottleneck | Horizontal scaling |
| **Replay** | Impossible | Can replay old messages |
| **Decoupling** | Tight coupling | Loose coupling |
| **Throughput** | ~1,000 msg/sec | 1,000,000+ msg/sec |

**When to Use Kafka:**
- âœ… High-volume data streams
- âœ… Multiple consumers need same data
- âœ… Need to replay or reprocess data
- âœ… Require fault tolerance
- âœ… Microservices architecture

**When NOT to Use Kafka:**
- âŒ Simple request-response (use REST API)
- âŒ Very low volume (< 100 messages/day)
- âŒ Need immediate consistency (use database transaction)
- âŒ Small project with 1 producer, 1 consumer

---

## 3. MACHINE LEARNING FOR ANOMALY DETECTION

### 3.1 What is Machine Learning?

**Machine Learning (ML)** = Teaching computers to learn from data instead of explicitly programming rules

**Traditional Programming:**
```
Rules (written by programmer) + Data = Output

Example:
IF temperature > 85 THEN "Too hot"
```

**Machine Learning:**
```
Data + Desired Output = Rules (learned by algorithm)

Example:
Input: 1000 sensor readings labeled normal/anomaly
Output: Algorithm learns patterns that indicate anomalies
```

**Why ML for Anomaly Detection?**

**Problem with rule-based systems:**
```
Rule: IF temperature > 85 THEN anomaly

But what if:
- 85Â°F is normal during summer?
- 75Â°F is abnormal at 3 AM?
- Temperature + vibration combined indicate bearing failure?
- Pattern of slowly rising temp over days matters?

â†’ Would need thousands of rules!
â†’ Rules don't capture complex patterns
â†’ Rules can't adapt to new situations
```

**Solution with ML:**
```
Training: Feed algorithm 1000 normal readings
Learning: Algorithm learns what "normal" looks like
Detection: New reading compared to learned patterns
Result: Catches complex anomalies humans would miss
```

### 3.2 Isolation Forest Explained

**Isolation Forest** is a machine learning algorithm specifically designed to find outliers (anomalies) in data.

**The Core Idea:**

Anomalies are:
1. **Few** (rare compared to normal)
2. **Different** (far from normal points)

Therefore, anomalies are **easier to isolate** than normal points.

**Visual Analogy:**

Imagine a forest where you randomly split data:

```
Normal Data (clustered together):
. . . . . . .
. . . X . . .  â† Hard to isolate
. . . . . . .
  (Many splits needed)

Anomalous Data (isolated):
. . . . . . .
. . . . . . .
. . . . . O â† Easy to isolate
  (Few splits needed)
```

**How It Works:**

**Step 1: Build Isolation Trees**
```
Start with all data points

Random Split 1: Choose random feature (e.g., temperature)
               Choose random value (e.g., 75Â°F)
               Split: temperature < 75 | temperature â‰¥ 75

Random Split 2: Choose random feature (e.g., vibration)
               Choose random value (e.g., 3 mm/s)
               Split: vibration < 3 | vibration â‰¥ 3

...continue until each point is isolated
```

**Step 2: Measure Path Length**
- Count splits needed to isolate each point
- **Anomalies**: Few splits (short path)
- **Normal**: Many splits (long path)

**Step 3: Calculate Anomaly Score**
- Average path length across 100 trees
- **Lower score** (shorter path) = More anomalous
- **Higher score** (longer path) = More normal

**Example in Rig Alpha:**

```python
# 50 features (sensor readings)
features = [temperature, pressure, humidity, ..., valve_position]

# Train on historical data
forest = IsolationForest(n_estimators=100, contamination=0.05)
forest.fit(normal_readings)

# New reading
new_reading = [85, 120, 45, ..., 87]

# Predict
prediction = forest.predict(new_reading)
# -1 = Anomaly
#  1 = Normal

anomaly_score = forest.decision_function(new_reading)
# More negative = More anomalous
```

**Key Parameters:**

**n_estimators** = Number of trees (default: 100)
- More trees = More accurate but slower
- Rig Alpha uses 100 trees (good balance)

**contamination** = Expected proportion of anomalies (default: 0.05 = 5%)
- Tells algorithm: "Expect about 5% of data to be outliers"
- Rig Alpha uses 5% based on industrial standards

**When Isolation Forest Works Best:**
- âœ… Large number of features (50 in Rig Alpha)
- âœ… Mix of normal and anomalous data
- âœ… Need fast real-time detection
- âœ… Point anomalies (single unusual readings)

**When It Doesn't Work Well:**
- âŒ Temporal patterns (use LSTM instead)
- âŒ Very small datasets (< 100 samples)
- âŒ All features are highly correlated

### 3.3 Z-Score Analysis

**Z-Score** = Number of standard deviations from the mean

**Formula:**
```
Z-Score = (Value - Mean) / Standard Deviation
```

**Interpretation:**
- Z = 0: Value is exactly at the mean (perfectly average)
- Z = 1: Value is 1 standard deviation above mean
- Z = -1: Value is 1 standard deviation below mean
- Z = 2: Value is 2 standard deviations above mean (unusual)
- Z = 3: Value is 3 standard deviations above mean (very unusual!)

**The 68-95-99.7 Rule:**
```
In a normal distribution:
- 68% of values within 1 standard deviation (Z: -1 to +1)
- 95% of values within 2 standard deviations (Z: -2 to +2)
- 99.7% of values within 3 standard deviations (Z: -3 to +3)

Therefore:
Z > 3 or Z < -3 â†’ Only 0.3% chance â†’ VERY UNUSUAL!
```

**Example in Rig Alpha:**

```
Historical bearing temperature data:
Mean: 100Â°F
Standard Deviation: 10Â°F

New Reading: 130Â°F

Z-Score = (130 - 100) / 10 = 3.0

Interpretation:
- 3 standard deviations above normal
- Only 0.15% chance of being normal
- LIKELY AN ANOMALY!
```

**How Rig Alpha Uses Z-Score:**

After Isolation Forest flags an anomaly, Z-score analysis identifies **which sensors** contributed:

```python
for sensor in all_sensors:
    value = reading[sensor]
    mean = historical_mean[sensor]
    std = historical_std[sensor]
    
    z_score = (value - mean) / std
    
    if abs(z_score) > 2.0:
        contributing_sensors.append(sensor)
```

**Example Output:**
```
Anomaly detected! Contributing sensors:
- bearing_temp: 130Â°F (Z-score: 3.2)
- vibration: 8.5 mm/s (Z-score: 2.8)
- lubrication_pressure: 15 PSI (Z-score: -2.4)

Interpretation:
â†’ Bearing is overheating
â†’ Excessive vibration
â†’ Lubrication pressure too low
â†’ Root cause: Lubrication system failure!
```

### 3.4 LSTM Autoencoder Explained

**LSTM** = **L**ong **S**hort-**T**erm **M**emory

**Autoencoder** = Neural network that learns to compress and reconstruct data

**LSTM Autoencoder** = Combines both for sequence learning

#### **What is LSTM?**

**The Problem:**
Regular neural networks can't remember sequences:
- Can't remember: "What was the temperature 5 minutes ago?"
- Can't detect: "Temperature has been rising for 3 hours"

**The Solution:**
LSTM has **memory cells** that remember previous inputs:

```
Time:    t=1     t=2     t=3     t=4     t=5
Input:   70Â°F â†’ 72Â°F â†’ 74Â°F â†’ 76Â°F â†’ 78Â°F
                â†“       â†“       â†“       â†“
LSTM:   [70] â†’ [70,72] â†’ [70,72,74] â†’ [70,72,74,76] â†’ [70,72,74,76,78]
         â†‘        â†‘            â†‘              â†‘                â†‘
      Memory   Memory       Memory        Memory          Memory
      
Detection: "Temperature rising consistently - bearing failure likely!"
```

**LSTM Architecture:**

```
LSTM Cell Components:

1. Forget Gate: What to forget from memory
   "Should I forget old temperatures?"

2. Input Gate: What new information to store
   "Should I remember this new temperature?"

3. Output Gate: What to output
   "Is this pattern normal or anomalous?"

4. Cell State: Long-term memory
   "History of temperature readings"
```

#### **What is an Autoencoder?**

**Goal:** Learn efficient representation of data

**Architecture:**
```
Input (50 sensors)
      â†“
Encoder: Compress to smaller representation
    [50 values] â†’ [32 values]
      â†“
Bottleneck (compressed representation)
      â†“
Decoder: Reconstruct original data
    [32 values] â†’ [50 values]
      â†“
Output (reconstructed 50 sensors)

Compare:
Original Input: [70, 120, 45, ..., 87]
Reconstructed:  [71, 119, 46, ..., 86]

Reconstruction Error = difference between them
```

**Key Insight:**
- Autoencoder trained on **normal data only**
- Learns to reconstruct normal patterns accurately
- **Anomalies**: Can't reconstruct well â†’ High error
- **Normal**: Reconstructs well â†’ Low error

**Example:**
```
Training (Normal data only):
Input:  [70, 120, 45]
Output: [70.1, 119.8, 45.2]
Error:  0.02 (very low - good reconstruction)

Testing (Anomaly):
Input:  [130, 150, 45]
Output: [75, 125, 45]
Error:  7.5 (very high - can't reconstruct anomaly!)
         â†‘
    ANOMALY DETECTED!
```

#### **LSTM Autoencoder Combined**

**Purpose:** Detect anomalies in **sequences** of sensor readings

**Architecture in Rig Alpha:**
```
Input Sequence: Last 20 sensor readings
[Reading t-19, Reading t-18, ..., Reading t-1, Reading t]

Each reading has 50 sensors:
[temp, pressure, humidity, ..., valve_position]

Total Input Shape: (20 timesteps, 50 features)

LSTM Encoder:
Layer 1: 128 LSTM units â†’ Learn high-level patterns
Layer 2: 64 LSTM units â†’ Learn mid-level patterns
Layer 3: 32 Dense units â†’ Compress to bottleneck

LSTM Decoder:
Layer 1: Repeat vector 20 times
Layer 2: 64 LSTM units â†’ Reconstruct patterns
Layer 3: 128 LSTM units â†’ Refine reconstruction
Output: (20 timesteps, 50 features)

Comparison:
Original Sequence: (20, 50)
Reconstructed: (20, 50)
MSE (Mean Squared Error) = Anomaly Score
```

**Training Process:**

**Step 1: Collect Normal Data**
```python
# Get 2000 normal readings from database
normal_readings = fetch_training_data(limit=2000)

# Create sequences of 20 readings
sequences = create_sequences(normal_readings, length=20)
# Shape: (1980, 20, 50)
#         â†‘    â†‘   â†‘
#      sequences timesteps features
```

**Step 2: Train Autoencoder**
```python
# Try to reconstruct input
model.fit(
    x=sequences,      # Input
    y=sequences,      # Try to match input (autoencoder)
    epochs=50,
    batch_size=32
)
```

**Step 3: Calculate Threshold**
```python
# Get reconstruction errors on training data
reconstructions = model.predict(sequences)
errors = mean_squared_error(sequences, reconstructions)

# Set threshold at 95th percentile
threshold = np.percentile(errors, 95)
# "Top 5% of errors = anomalies"
```

**Detection Process:**

```python
# Get last 20 readings
recent_sequence = get_recent_sequence(reading_id)

# Reconstruct
reconstruction = model.predict(recent_sequence)

# Calculate error
mse = mean_squared_error(recent_sequence, reconstruction)

# Detect
if mse > threshold:
    is_anomaly = True
    # Identify which sensors have high reconstruction error
    contributing_sensors = identify_sensors_with_high_error()
else:
    is_anomaly = False
```

**Example Detection:**

```
Normal Pattern:
Time:  t-20  t-19  t-18  ...  t-1   t
Temp:   70    71    70   ...   71   70
                â†“
Reconstruction:
Temp:   70.2  70.8  70.1 ...  70.9  70.3
MSE: 0.02 â†’ Normal

Anomalous Pattern:
Time:  t-20  t-19  t-18  ...  t-1   t
Temp:   70    72    75   ...   90   95
                â†“
Reconstruction:
Temp:   70.5  71.2  72.1 ...  73.5  74.2
MSE: 8.7 â†’ ANOMALY!

Interpretation:
â†’ LSTM learned: "Temperature should stay around 70-72Â°F"
â†’ Actual: Temperature rising rapidly
â†’ Can't reconstruct this unusual pattern
â†’ High error indicates bearing failure
```

**Key Parameters in Rig Alpha:**

**LSTM_SEQUENCE_LENGTH = 20**
- Analyzes last 20 readings (20 Ã— 1 second = 20 seconds of history)
- Longer = Better at catching gradual trends
- Shorter = Faster response to sudden changes

**LSTM_ENCODING_DIM = 32**
- Bottleneck size (compress 50 features â†’ 32)
- Smaller = More compression, might miss details
- Larger = Less compression, might overfit

**LSTM_THRESHOLD_PERCENTILE = 95**
- Top 5% of reconstruction errors flagged as anomalies
- Lower (e.g., 90) = More sensitive, more false alarms
- Higher (e.g., 99) = Less sensitive, might miss anomalies

**When LSTM Works Best:**
- âœ… Temporal patterns (gradual degradation)
- âœ… Sequence dependencies (A affects B affects C)
- âœ… Slow-developing failures
- âœ… Context matters (3 AM vs 3 PM)

**When Isolation Forest Works Better:**
- âœ… Point anomalies (sudden spikes)
- âœ… Faster detection (no need for sequence)
- âœ… Less training data needed
- âœ… More interpretable

### 3.5 Hybrid Detection Strategy

**The Problem:**

Both Isolation Forest and LSTM have strengths and weaknesses:

| Scenario | Isolation Forest | LSTM |
|----------|------------------|------|
| Sudden spike | âœ… Excellent | âŒ Might miss |
| Gradual rise over days | âŒ Might miss | âœ… Excellent |
| Need fast response | âœ… Instant | âŒ Needs sequence |
| Complex temporal patterns | âŒ Misses context | âœ… Captures context |

**The Solution: Hybrid Approach**

Rig Alpha combines both methods with configurable strategies:

**Strategy 1: isolation_forest (Default)**
```
Only use Isolation Forest
+ Fast
+ Works immediately
+ Good for sudden anomalies
- Misses gradual degradation
```

**Strategy 2: lstm**
```
Only use LSTM
+ Excellent for temporal patterns
+ Catches gradual issues
- Needs 100+ readings to start
- Slower
```

**Strategy 3: hybrid_or**
```
Flag if EITHER method detects

Isolation Forest: Normal   â”
LSTM: Anomaly              â”‚â†’ Flag as ANOMALY
                           
Isolation Forest: Anomaly  â”
LSTM: Normal               â”‚â†’ Flag as ANOMALY

+ Maximum sensitivity
- More false positives
```

**Strategy 4: hybrid_and**
```
Flag only if BOTH agree

Isolation Forest: Anomaly  â”
LSTM: Anomaly              â”‚â†’ Flag as ANOMALY

Isolation Forest: Normal   â”
LSTM: Normal               â”‚â†’ Normal

+ High confidence
+ Fewer false positives
- Might miss some anomalies
```

**Strategy 5: hybrid_smart (Recommended)**
```
Isolation Forest primary, LSTM catches what IF misses

Step 1: Check Isolation Forest
  If anomaly â†’ FLAG immediately

Step 2: If IF says normal, check LSTM
  If LSTM says anomaly â†’ FLAG (gradual issue)

+ Best of both worlds
+ Fast response to sudden issues
+ Catches gradual degradation
+ Reasonable false positive rate
```

**Configuration in Rig Alpha:**
```python
# In config.py
HYBRID_DETECTION_STRATEGY = 'hybrid_smart'

# Options:
# 'isolation_forest' - Fast, point-based
# 'lstm' - Temporal patterns only
# 'hybrid_or' - Either flags = anomaly
# 'hybrid_and' - Both must flag
# 'hybrid_smart' - IF primary, LSTM backup
```

**Real-World Example:**

```
Scenario: Bearing Failure

Day 1-3 (Gradual temperature rise):
Isolation Forest: Normal (within normal range)
LSTM: ANOMALY (unusual upward trend)
â†’ hybrid_smart: FLAG (LSTM caught it!)
â†’ Action: Schedule bearing inspection

Day 4 (Sudden temperature spike):
Isolation Forest: ANOMALY (way outside normal)
LSTM: ANOMALY (continuing upward trend)
â†’ hybrid_smart: FLAG (both agree!)
â†’ Action: EMERGENCY - Stop machine NOW!

Result: Prevented catastrophic failure
```

---

## 4. AI AND NATURAL LANGUAGE PROCESSING

### 4.1 What is Artificial Intelligence (AI)?

**AI** = Computer systems that can perform tasks normally requiring human intelligence

**Types of AI:**

**1. Narrow AI (Weak AI)**
- Designed for specific tasks
- Example: Rig Alpha's anomaly detection
- **This is what exists today**

**2. General AI (Strong AI)**
- Human-level intelligence across all domains
- Can learn any task a human can
- **Does not exist yet (future technology)**

**AI in Rig Alpha:**
- âŒ NOT sentient or conscious
- âŒ NOT making independent decisions
- âœ… Pattern recognition (ML models)
- âœ… Natural language generation (reports)
- âœ… Data analysis and correlation

### 4.2 What is Natural Language Processing (NLP)?

**NLP** = AI technology that understands and generates human language

**Examples:**
- **Understanding**: Siri understanding "Set alarm for 7 AM"
- **Generation**: ChatGPT writing an essay
- **Translation**: Google Translate
- **Sentiment**: Detecting if a review is positive/negative

**In Rig Alpha:**
- **Input**: Structured sensor data and anomaly details
- **Output**: Human-readable analysis report

**Example:**
```
Input (Data):
{
  "anomaly_score": -0.08,
  "detected_sensors": ["bearing_temp", "vibration"],
  "bearing_temp": 165,
  "vibration": 7.2,
  "baseline_bearing_temp": 120,
  "baseline_vibration": 2.5
}

Output (Natural Language):
"An anomaly has been detected in the bearing system.
The bearing temperature has risen to 165Â°F, which is
45Â°F above the normal baseline of 120Â°F. Additionally,
vibration levels have increased to 7.2 mm/s, nearly
triple the expected 2.5 mm/s. This pattern is consistent
with bearing degradation due to insufficient lubrication.

Recommended Actions:
1. Inspect bearing lubrication system immediately
2. Check oil pressure and flow rate
3. Schedule bearing replacement within 48 hours
4. Monitor vibration closely until repaired"
```

### 4.3 Large Language Models (LLMs)

**LLM** = Large Language Model

**Definition:** AI models trained on massive amounts of text to understand and generate human-like language

**"Large" means:**
- Billions of parameters (GPT-3: 175 billion parameters)
- Trained on hundreds of billions of words
- Requires significant computational resources

**Famous LLMs:**
- **GPT-4** (OpenAI) - ChatGPT
- **LLaMA** (Meta) - Open source
- **Claude** (Anthropic)
- **Gemini** (Google)

**How LLMs Work (Simplified):**

**Training Phase:**
```
1. Feed model massive text corpus:
   - Books, articles, websites, code
   - Example: "All of Wikipedia"

2. Model learns patterns:
   - Grammar rules
   - Common phrases
   - Domain knowledge
   - Writing styles

3. Result: Model can predict next word given context
```

**Generation Phase:**
```
Input: "The bearing temperature is"
Model predicts: "rising" (most likely next word)

Input: "The bearing temperature is rising"
Model predicts: "rapidly" (next word)

Continue until complete response...
```

**Why LLMs for Rig Alpha:**

**Without LLM:**
```python
if bearing_temp > 150:
    print("Bearing temperature high")
```
- Limited, rigid responses
- Can't explain "why"
- No contextual recommendations

**With LLM:**
```python
prompt = build_complex_prompt(anomaly_data)
response = llm.generate(prompt)
```
- Detailed, contextual explanations
- Root cause analysis
- Specific actionable recommendations
- Considers multiple factors simultaneously

### 4.4 Groq Explained

**Groq** = AI infrastructure company (not a typo for "Grok")

**What Groq Provides:**
- Ultra-fast inference for LLMs
- API access to models like LLaMA
- Faster than traditional GPU inference
- Free tier for developers

**Groq vs OpenAI:**

| Feature | Groq | OpenAI |
|---------|------|--------|
| **Speed** | Extremely fast | Standard |
| **Models** | LLaMA, Mixtral | GPT-3.5, GPT-4 |
| **Cost** | Free tier available | Pay per token |
| **API** | OpenAI-compatible | Native |

**Why Rig Alpha Uses Groq:**
1. **Speed**: Faster report generation
2. **Cost**: Free for development
3. **Open Models**: LLaMA is open source
4. **Compatibility**: Uses OpenAI API format

**Groq API in Rig Alpha:**

```python
from openai import OpenAI

# Initialize Groq client (uses OpenAI library)
client = OpenAI(
    api_key="gsk_...",              # Groq API key
    base_url="https://api.groq.com/openai/v1"
)

# Generate report
response = client.chat.completions.create(
    model="llama-3.3-70b-versatile",
    messages=[
        {"role": "system", "content": "You are an expert..."},
        {"role": "user", "content": "Analyze this anomaly..."}
    ],
    temperature=0.7,
    max_tokens=4000
)
```

### 4.5 LLaMA Explained

**LLaMA** = **L**arge **L**anguage **M**odel **M**eta **A**I

**Created by:** Meta (Facebook) in 2023

**Key Features:**
- Open source (unlike GPT-4)
- Multiple sizes: 7B, 13B, 70B parameters
- Efficient architecture
- Good performance vs size ratio

**LLaMA Models in Groq:**

**llama-3.3-70b-versatile:**
- 70 billion parameters
- Best for complex analysis
- Used in Rig Alpha for detailed reports
- Slower but more accurate

**llama-3.1-8b-instant:**
- 8 billion parameters
- Fast responses
- Could be used for quick summaries
- Less detailed than 70B

**Training:**
- Trained on 2 trillion tokens
- Includes scientific papers, documentation, code
- Understands technical concepts
- Can explain complex topics simply

**Why LLaMA for Industrial Applications:**
1. **Domain Knowledge**: Trained on technical documentation
2. **Reasoning**: Can connect cause and effect
3. **Open**: No vendor lock-in
4. **Cost-Effective**: Efficient architecture

### 4.6 Prompt Engineering

**Prompt** = Instructions given to an LLM

**Quality of output depends heavily on prompt quality**

**Bad Prompt:**
```
"Analyze this anomaly"
```
- Too vague
- No context
- Poor results

**Good Prompt (Rig Alpha Style):**
```
You are an industrial sensor data analyst expert.
Analyze the following anomaly detected in a
manufacturing/industrial sensor system.

ANOMALY DETECTION DETAILS:
- Detection Method: isolation_forest
- Anomaly Score: -0.0842
- Timestamp: 2026-01-12T15:30:00
- Primary Sensors Flagged: bearing_temp, vibration

SENSOR READINGS AT ANOMALY:
The system monitors 50 parameters...

TOP 10 PARAMETER DEVIATIONS:
1. bearing_temp: 165Â°F (baseline: 120Â°F, z-score: 3.2)
2. vibration: 7.2 mm/s (baseline: 2.5 mm/s, z-score: 2.8)
...

Based on this data, provide:
1. ROOT CAUSE ANALYSIS
2. AFFECTED SYSTEMS
3. SEVERITY ASSESSMENT
4. PREVENTION RECOMMENDATIONS
5. IMMEDIATE ACTIONS REQUIRED
```

**Key Elements of Good Prompts:**

1. **Role Definition**
   - "You are an expert in..."
   - Sets context and tone

2. **Structured Data**
   - Organized clearly
   - Labels and categories

3. **Specific Instructions**
   - "Provide a list of..."
   - "Rate severity as..."

4. **Format Requirements**
   - "Format in markdown"
   - "Use bullet points"

5. **Context**
   - Background information
   - Domain-specific details

**Rig Alpha's Prompt Strategy:**

```python
def _build_prompt(anomaly_data, analysis_summary):
    # 1. Set expert role
    role = "You are an industrial sensor analyst..."
    
    # 2. Provide detection details
    detection_info = f"Score: {score}, Method: {method}"
    
    # 3. List sensor deviations with z-scores
    deviations = format_deviations(analysis_summary)
    
    # 4. Show correlations
    correlations = format_correlations(analysis_summary)
    
    # 5. Request structured output
    request = """
    Provide analysis with these sections:
    1. Root Cause Analysis
    2. Affected Systems
    3. Severity Assessment
    4. Prevention Recommendations
    5. Immediate Actions
    """
    
    return f"{role}\n\n{detection_info}\n\n{deviations}\n\n{correlations}\n\n{request}"
```

**Temperature Parameter:**
```python
temperature=0.7  # Controls randomness

0.0 = Deterministic (same input = same output)
0.5 = Slightly creative
0.7 = Balanced (Rig Alpha uses this)
1.0 = More creative/varied
2.0 = Very random
```

**Max Tokens:**
```python
max_tokens=4000  # Maximum length of response

1 token â‰ˆ 0.75 words
4000 tokens â‰ˆ 3000 words
```

### 4.7 Fallback Strategy

**Problem:** What if Groq API is unavailable or no API key?

**Rig Alpha's Solution:** Data-driven fallback reports

Instead of generic "API unavailable" message, generate reports from data:

```python
def _generate_fallback_report(analysis_summary, anomaly_data):
    # Extract real data
    top_deviations = analysis_summary['patterns']['top_deviations']
    correlations = analysis_summary['correlations']['significant']
    
    # Calculate severity from z-scores
    max_z_score = max([dev['z_score'] for dev in top_deviations])
    if max_z_score > 3.0:
        severity = "CRITICAL"
    elif max_z_score > 2.5:
        severity = "HIGH"
    else:
        severity = "MEDIUM"
    
    # Build report from data
    report = f"""
    ## Automated Analysis Report
    
    ### Severity: {severity}
    
    ### Top Deviations Detected:
    {format_deviations(top_deviations)}
    
    ### Sensor Correlations:
    {format_correlations(correlations)}
    
    ### Recommended Actions:
    {generate_actions(severity, top_deviations)}
    """
    
    return report
```

**Benefits:**
- Still useful without API key
- Based on actual sensor data
- Reliable and deterministic
- Good for development/testing

---

## 5. DATABASE TECHNOLOGIES

### 5.1 What is PostgreSQL?

**PostgreSQL** = Open-source relational database management system

**"Postgres" for short**

**Relational Database = Data organized in tables (like Excel spreadsheets)**

**Example:**
```
Table: sensor_readings
+----+-----------+-------------+-------------+-----------+
| id | timestamp | temperature | pressure    | machine   |
+----+-----------+-------------+-------------+-----------+
| 1  | 10:00:01  | 70.5        | 120.3       | A         |
| 2  | 10:00:02  | 71.2        | 119.8       | A         |
| 3  | 10:00:03  | 70.8        | 120.1       | B         |
+----+-----------+-------------+-------------+-----------+
```

**Why PostgreSQL vs Other Databases:**

| Feature | PostgreSQL | MySQL | MongoDB |
|---------|-----------|-------|---------|
| **Type** | Relational | Relational | Document (NoSQL) |
| **ACID** | âœ… Full | âœ… Full | âš ï¸ Limited |
| **JSON** | âœ… JSONB | âš ï¸ Basic | âœ… Native |
| **Complex Queries** | âœ… Excellent | âœ… Good | âŒ Limited |
| **Extensions** | âœ… Many | âš ï¸ Some | âŒ Few |
| **Open Source** | âœ… Truly free | âš ï¸ Owned by Oracle | âœ… Free |

**Why Rig Alpha Uses PostgreSQL:**
1. **JSONB Support**: Custom sensors stored as JSON
2. **Reliability**: ACID compliance (data never lost)
3. **Performance**: Handles millions of rows
4. **Time-Series**: Good for sensor data over time
5. **Free**: No licensing costs

### 5.2 ACID Compliance

**ACID** = Properties that guarantee database reliability

**A = Atomicity**
- **Definition**: Transaction completes fully or not at all
- **Example**: 
  ```
  Transaction: Insert sensor reading + Update statistics
  
  Scenario 1: Both succeed âœ…
  Scenario 2: Both fail (rolled back) âœ…
  Scenario 3: One succeeds, one fails âŒ IMPOSSIBLE
  ```
- **Why it matters**: No partial data corruption

**C = Consistency**
- **Definition**: Database always in valid state
- **Example**:
  ```
  Rule: bearing_temp must be between 70-180Â°F
  
  Try to insert: bearing_temp = 500Â°F
  Result: REJECTED (violates constraint)
  ```
- **Why it matters**: Data integrity guaranteed

**I = Isolation**
- **Definition**: Concurrent transactions don't interfere
- **Example**:
  ```
  Transaction A: Read count (1000) â†’ Increment â†’ Write (1001)
  Transaction B: Read count (1000) â†’ Increment â†’ Write (1001)
  
  Without Isolation: Final count = 1001 (lost update!)
  With Isolation: Final count = 1002 (correct!)
  ```
- **Why it matters**: Accurate in multi-user systems

**D = Durability**
- **Definition**: Committed data survives crashes
- **Example**:
  ```
  1. Write sensor reading to database
  2. Database confirms "Committed!"
  3. Power failure 1 second later
  4. Restart database
  5. Data is still there âœ…
  ```
- **Why it matters**: Data never lost after commit

### 5.3 Connection Pooling

**The Problem:**

Creating database connections is slow:
```
Time to create connection: 50-100ms
Time to execute query: 5ms

Without pooling:
1. Create connection (50ms)
2. Execute query (5ms)
3. Close connection
4. Repeat for every request...

Total: 55ms per query (90% overhead!)
```

**The Solution: Connection Pool**

```
Startup:
1. Create 10 connections
2. Keep them open and ready

When query needed:
1. Borrow connection from pool (instant)
2. Execute query (5ms)
3. Return connection to pool

Total: 5ms per query (11x faster!)
```

**Rig Alpha's Pool Configuration:**

```python
db_pool = pool.ThreadedConnectionPool(
    minconn=5,   # Always keep 5 connections ready
    maxconn=50,  # Maximum 50 concurrent connections
    **DB_CONFIG
)

# Usage
conn = db_pool.getconn()      # Borrow
cursor = conn.cursor()
cursor.execute("SELECT ...")  # Use
db_pool.putconn(conn)         # Return (DON'T CLOSE!)
```

**Benefits:**
- **Speed**: 50% faster response times
- **Scalability**: Handle more concurrent requests
- **Reliability**: Connections tested before use
- **Resource Management**: Prevents connection exhaustion

**Common Mistake:**
```python
# âŒ DON'T DO THIS
conn = db_pool.getconn()
# ...
conn.close()  # WRONG! This closes permanently!

# âœ… DO THIS
conn = db_pool.getconn()
# ...
db_pool.putconn(conn)  # Returns to pool for reuse
```

### 5.4 JSONB in PostgreSQL

**JSON** = **J**ava**S**cript **O**bject **N**otation (text format for data)

**JSONB** = **JSON** **B**inary (PostgreSQL's optimized format)

**Why JSONB for Custom Sensors:**

**Problem:**
- System has 50 built-in sensors
- Users want to add custom sensors
- Each user might add different sensors
- Can't add new column for each custom sensor (too rigid)

**Solution:**
- Store custom sensors in JSONB column
- Flexible: Any structure
- Queryable: Can search inside JSON
- Efficient: Binary storage

**Example:**

```sql
-- Table structure
CREATE TABLE sensor_readings (
    id SERIAL PRIMARY KEY,
    timestamp TIMESTAMP,
    -- 50 regular columns
    temperature FLOAT,
    pressure FLOAT,
    ...
    -- Flexible custom sensors column
    custom_sensors JSONB
);

-- Insert with custom sensors
INSERT INTO sensor_readings VALUES (
    1,
    NOW(),
    70.5,  -- temperature
    120.3, -- pressure
    ...
    '{"oil_quality": 95, "custom_vibration_x": 2.3}'::JSONB
);

-- Query custom sensor
SELECT * FROM sensor_readings
WHERE custom_sensors->>'oil_quality' > '90';
```

**JSONB Operators:**

```sql
-- Extract value
custom_sensors->>'oil_quality'  -- Returns: "95" (as text)
custom_sensors->'oil_quality'   -- Returns: 95 (as JSON)

-- Check if key exists
custom_sensors ? 'oil_quality'  -- Returns: true

-- Containment
custom_sensors @> '{"oil_quality": 95}'  -- Returns: true

-- Get all keys
jsonb_object_keys(custom_sensors)  -- Returns: ["oil_quality", "custom_vibration_x"]
```

**Performance:**
- JSONB is indexed (fast queries)
- Binary format (smaller storage)
- Validated on insert (catches errors)

### 5.5 Neon.tech - Serverless Postgres

**Neon** = Managed PostgreSQL service in the cloud

**"Serverless" = You don't manage servers**

**Traditional Database:**
```
You must:
- Set up server
- Install Postgres
- Configure security
- Manage backups
- Handle scaling
- Monitor performance
- Apply updates

Cost: 24/7 server running
```

**Serverless (Neon):**
```
Neon handles:
- Server setup âœ…
- Security âœ…
- Automatic backups âœ…
- Instant scaling âœ…
- Monitoring âœ…
- Updates âœ…

Cost: Pay only for storage + compute used
```

**Key Features:**

**1. Branching**
```
Like Git for databases:
- Create branch for testing
- Make changes safely
- Merge or discard
```

**2. Instant Scaling**
```
Low traffic: 0.25 vCPU
High traffic: Auto-scale to 4 vCPU
No manual intervention!
```

**3. Auto-Pause**
```
No activity for 5 minutes?
â†’ Database pauses
â†’ No compute charges
â†’ Instant resume on next query
```

**4. Point-in-Time Recovery**
```
"Undo" database to any point in last 7 days
Recover from accidental deletion
```

**Connection String:**
```
postgresql://username:password@ep-xxx-123.us-east-2.aws.neon.tech/neondb?sslmode=require
```

**Why Rig Alpha Supports Neon:**
1. **Easy Deployment**: No server setup
2. **Free Tier**: Good for development
3. **Global**: Deploy anywhere
4. **Reliable**: Built-in redundancy
5. **Fast**: Optimized Postgres

---

## 6. FRONTEND AND 3D VISUALIZATION

### 6.1 Web Technologies Overview

**Frontend** = What runs in the user's web browser

**Technology Stack:**

```
Browser
    â”œâ”€â”€ HTML (Structure)
    â”œâ”€â”€ CSS (Styling)
    â””â”€â”€ JavaScript (Behavior)
        â””â”€â”€ React (UI Library)
            â”œâ”€â”€ React Three Fiber (3D Graphics)
            â”œâ”€â”€ Zustand (State Management)
            â””â”€â”€ Socket.IO (Real-time Communication)
```

### 6.2 React Explained

**React** = JavaScript library for building user interfaces

**Created by:** Facebook (now Meta) in 2013

**Key Concept: Components**

Instead of one big HTML page, break UI into reusable pieces:

```jsx
// Traditional HTML (all in one file)
<div>
  <h1>Dashboard</h1>
  <div class="sensor-card">Temperature: 70Â°F</div>
  <div class="sensor-card">Pressure: 120 PSI</div>
  <div class="sensor-card">RPM: 3000</div>
</div>

// React (reusable components)
<Dashboard>
  <SensorCard sensor="temperature" value={70} unit="Â°F" />
  <SensorCard sensor="pressure" value={120} unit="PSI" />
  <SensorCard sensor="rpm" value={3000} unit="RPM" />
</Dashboard>
```

**Benefits:**
- **Reusability**: Write once, use everywhere
- **Maintainability**: Change component, updates everywhere
- **Performance**: Only re-render what changed

**React State:**

```jsx
// State = Data that can change
const [temperature, setTemperature] = useState(70);

// When state changes, UI updates automatically
setTemperature(85);  // UI shows 85Â°F instantly
```

### 6.3 React Three Fiber

**Three.js** = JavaScript library for 3D graphics

**React Three Fiber** = React wrapper for Three.js

**Why 3D Visualization?**

**2D Dashboard:**
- Shows numbers and charts
- Requires interpretation
- Less intuitive

**3D Digital Twin:**
- Visual representation of machines
- Immediately see what's wrong
- Intuitive understanding

**Example:**
```jsx
// 3D Rig with spinning turbine
<RigModel position={[0, 0, 0]}>
  <Turbine rotation={[0, rpm * 0.01, 0]} />
  <Bearing temperature={165} />
  {anomaly && <WarningLight />}
</RigModel>
```

**Key Concepts:**

**Scene:**
- 3D space containing all objects
- Like a stage in theater

**Mesh:**
- 3D object (geometry + material)
- Example: Turbine, pipe, floor

**Camera:**
- Viewpoint into the scene
- First-person in Rig Alpha

**Lights:**
- Illuminate the scene
- Ambient, directional, point lights

**Animation:**
- Update object properties every frame
- 60 FPS = 60 updates per second

### 6.4 WebGL and Three.js

**WebGL** = **Web** **G**raphics **L**ibrary

**Definition:** JavaScript API for rendering 3D graphics in browser without plugins

**How It Works:**

```
1. JavaScript sends drawing commands
2. WebGL translates to GPU instructions
3. GPU renders pixels
4. Displayed in HTML <canvas> element
```

**Three.js Abstracts Complexity:**

**Raw WebGL (Complex):**
```javascript
// Hundreds of lines to draw a cube
var buffer = gl.createBuffer();
gl.bindBuffer(gl.ARRAY_BUFFER, buffer);
var vertices = [/* 24 coordinates */];
gl.bufferData(gl.ARRAY_BUFFER, new Float32Array(vertices), gl.STATIC_DRAW);
// ... 50 more lines ...
```

**Three.js (Simple):**
```javascript
// One line to draw a cube
const cube = new THREE.BoxGeometry(1, 1, 1);
```

**React Three Fiber (Even Simpler):**
```jsx
<Box />
```

### 6.5 WebSocket and Socket.IO

**WebSocket** = Protocol for real-time bidirectional communication

**Traditional HTTP:**
```
Client: "Any updates?" â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Server
Client: â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ "Nope"
(Wait 1 second)
Client: "Any updates now?" â”€â”€â”€â”€â†’ Server
Client: â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ "Still nope"
(Inefficient polling!)
```

**WebSocket:**
```
Client: "Connect" â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Server
Server: "Connected" â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Client
(Connection stays open)

When data available:
Server: "Here's new data!" â”€â”€â”€â”€â”€â†’ Client
(Instant push!)
```

**Socket.IO** = Library making WebSocket easy

**Features:**
- Automatic reconnection
- Room support (different channels)
- Event-based messaging
- Fallback to HTTP polling if WebSocket unavailable

**In Rig Alpha:**

**Server (dashboard.py):**
```python
from flask_socketio import SocketIO

socketio = SocketIO(app)

# Broadcast telemetry every 100ms
def broadcast_telemetry():
    while True:
        data = get_current_telemetry()
        socketio.emit('rig_telemetry', data)
        time.sleep(0.1)  # 10 Hz
```

**Client (useSocket.js):**
```javascript
import io from 'socket.io-client';

const socket = io('http://localhost:5000');

// Receive telemetry
socket.on('rig_telemetry', (data) => {
    updateSensorStore(data);  // Update 3D scene
});
```

**Result:**
- Sensor data flows continuously
- 3D visualization updates in real-time
- No polling overhead
- Low latency (< 100ms)

### 6.6 Zustand State Management

**Problem:** Managing state in React

**Traditional Approach (Prop Drilling):**
```jsx
// Pass data through many components
<App data={sensorData}>
  <Scene data={sensorData}>
    <Factory data={sensorData}>
      <Rig data={sensorData}>
        <Turbine rpm={sensorData.rpm} />
      </Rig>
    </Factory>
  </Scene>
</App>
```
- Cumbersome
- Hard to maintain
- Performance issues

**Zustand Solution: Global Store**
```jsx
// Create store
const useSensorStore = create((set) => ({
  rpm: 0,
  temperature: 70,
  updateRPM: (rpm) => set({ rpm })
}));

// Use anywhere
function Turbine() {
  const rpm = useSensorStore((state) => state.rpm);
  return <mesh rotation={[0, rpm * 0.01, 0]} />;
}
```

**Benefits:**
- No prop drilling
- Simple API
- Good performance
- Small bundle size

**Transient Updates Pattern:**

**Problem:** 10 Hz updates cause 10 re-renders per second = Laggy

**Solution:** Update state without triggering re-renders

```javascript
// Regular update (triggers re-render)
set({ rpm: newRPM });  // âŒ Slow for high-frequency updates

// Transient update (no re-render)
useSensorStore.setState({ rpm: newRPM });  // âœ… Fast!

// Read in animation loop (60 FPS)
useFrame(() => {
  const rpm = useSensorStore.getState().rpm;
  mesh.rotation.y = rpm * 0.01;
});
```

**Result:**
- 10 Hz WebSocket updates (smooth)
- 60 FPS rendering (smooth)
- Zero performance bottleneck

### 6.7 PBR (Physically Based Rendering)

**PBR** = Rendering technique simulating real-world light physics

**Properties:**

**1. Metalness**
- 0 = Non-metal (wood, plastic)
- 1 = Metal (steel, aluminum)

**2. Roughness**
- 0 = Smooth/shiny (mirror)
- 1 = Rough/matte (concrete)

**3. Color (Base Color)**
- Surface color in white light

**Example:**
```jsx
<meshStandardMaterial
  color="#888888"      // Gray
  metalness={0.8}      // Mostly metal
  roughness={0.3}      // Slightly shiny
  emissive="#ff0000"   // Glows red when hot
  emissiveIntensity={tempNormalized}  // Brightness based on temp
/>
```

**Why PBR for Industrial Visualization:**
- Looks realistic
- Intuitive understanding
- Metallic machinery looks correct
- Temperature glow looks natural

### 6.8 Post-Processing Effects

**Post-Processing** = Effects applied after scene is rendered

**Like Instagram filters for 3D graphics**

**Bloom Effect:**
```
Normal rendering: Bright areas are just bright
With bloom: Bright areas glow and bleed

Example: Hot bearing
Temperature: 165Â°F
â†’ Orange emissive color
â†’ Bloom makes it glow realistically
â†’ Immediately visible from distance
```

**Vignette Effect:**
```
Darkens edges of screen
Focuses attention on center
Common in cinematic looks
```

**How It Works:**
```
1. Render scene to texture
2. Apply bloom shader (find bright pixels, blur them)
3. Apply vignette shader (darken edges)
4. Display final result
```

**Configuration:**
```jsx
<EffectComposer>
  <Bloom
    intensity={1.5}        // How strong
    luminanceThreshold={0.9}  // How bright to bloom
    luminanceSmoothing={0.9}  // Blur amount
  />
  <Vignette
    offset={0.5}          // Start distance from center
    darkness={0.5}        // How dark
  />
</EffectComposer>
```

### 6.9 First-Person Controls

**Pointer Lock API:**
- Captures mouse movement
- Hides cursor
- Prevents leaving window

**WASD Movement:**
```javascript
const keys = { w: false, a: false, s: false, d: false };

document.addEventListener('keydown', (e) => {
  if (e.key === 'w') keys.w = true;
  // ... etc
});

useFrame(() => {
  const velocity = new THREE.Vector3();
  
  if (keys.w) velocity.z -= 1;   // Forward
  if (keys.s) velocity.z += 1;   // Backward
  if (keys.a) velocity.x -= 1;   // Left
  if (keys.d) velocity.x += 1;   // Right
  
  camera.position.add(velocity);
});
```

**Mouse Look:**
```javascript
document.addEventListener('mousemove', (e) => {
  camera.rotation.y -= e.movementX * 0.002;
  camera.rotation.x -= e.movementY * 0.002;
  
  // Clamp vertical rotation (can't look behind)
  camera.rotation.x = Math.max(-Math.PI/2, Math.min(Math.PI/2, camera.rotation.x));
});
```

---

## ğŸ“ SUMMARY AND NEXT STEPS

You've now learned the fundamental concepts behind every technology in Rig Alpha:

âœ… **Industrial IoT** - What sensors are and why monitoring matters  
âœ… **Apache Kafka** - Message streaming for real-time data  
âœ… **Machine Learning** - Isolation Forest and LSTM for anomaly detection  
âœ… **AI/NLP** - How Groq and LLaMA generate analysis reports  
âœ… **PostgreSQL** - Database storage with JSONB and connection pooling  
âœ… **3D Visualization** - React Three Fiber for digital twin

**Next Document:** [02-ARCHITECTURE.md](02-ARCHITECTURE.md)
- See how all these technologies work together
- Understand data flow through the system
- Learn about each layer in detail

---

*Continue to Document 02: System Architecture â†’*
